# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HEALTHCP: Provision Infrastructure
# Creates EKS clusters with hub-and-spoke naming convention
# Hub: argocd-use1 | Spoke: opsera-use1-np
# Generated by Opsera Code-to-Cloud Enterprise v1.4
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

name: "ðŸ—ï¸ Provision Infrastructure"

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - plan
          - apply
          - destroy
        default: plan
      create_hub:
        description: 'Create Hub cluster (argocd-use1)'
        type: boolean
        default: true
      create_spoke:
        description: 'Create Spoke cluster (opsera-use1-np)'
        type: boolean
        default: true

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.6.0
  # Cluster naming convention
  HUB_CLUSTER: argocd-use1
  SPOKE_CLUSTER: opsera-use1-np

jobs:
  provision:
    name: "ðŸ—ï¸ ${{ inputs.action }} Infrastructure"
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Check Existing Clusters
        id: check
        run: |
          echo "### ðŸ” Checking Existing Clusters" >> $GITHUB_STEP_SUMMARY
          
          HUB_EXISTS=$(aws eks describe-cluster --name ${{ env.HUB_CLUSTER }} 2>/dev/null && echo "true" || echo "false")
          SPOKE_EXISTS=$(aws eks describe-cluster --name ${{ env.SPOKE_CLUSTER }} 2>/dev/null && echo "true" || echo "false")
          
          echo "hub_exists=$HUB_EXISTS" >> $GITHUB_OUTPUT
          echo "spoke_exists=$SPOKE_EXISTS" >> $GITHUB_OUTPUT
          
          echo "| Cluster | Exists |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ env.HUB_CLUSTER }} | $HUB_EXISTS |" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ env.SPOKE_CLUSTER }} | $SPOKE_EXISTS |" >> $GITHUB_STEP_SUMMARY

      - name: Generate Terraform Configuration
        run: |
          mkdir -p /tmp/terraform
          cd /tmp/terraform
          
          cat > main.tf << 'EOF'
          terraform {
            required_version = ">= 1.0"
            required_providers {
              aws = {
                source  = "hashicorp/aws"
                version = "~> 5.0"
              }
            }
          }

          provider "aws" {
            region = var.aws_region
          }

          variable "aws_region" {
            default = "us-east-1"
          }

          variable "create_hub" {
            type    = bool
            default = true
          }

          variable "create_spoke" {
            type    = bool
            default = true
          }

          # Data sources
          data "aws_availability_zones" "available" {
            filter {
              name   = "opt-in-status"
              values = ["opt-in-not-required"]
            }
          }

          data "aws_caller_identity" "current" {}

          # Use existing VPC or find default
          data "aws_vpcs" "existing" {}

          data "aws_vpc" "selected" {
            id = tolist(data.aws_vpcs.existing.ids)[0]
          }

          data "aws_subnets" "private" {
            filter {
              name   = "vpc-id"
              values = [data.aws_vpc.selected.id]
            }
            filter {
              name   = "tag:Name"
              values = ["*private*", "*Private*"]
            }
          }

          data "aws_subnets" "public" {
            filter {
              name   = "vpc-id"
              values = [data.aws_vpc.selected.id]
            }
            filter {
              name   = "tag:Name"
              values = ["*public*", "*Public*"]
            }
          }

          locals {
            hub_cluster_name   = "argocd-use1"
            spoke_cluster_name = "opsera-use1-np"
            
            # Use discovered subnets or fall back to defaults
            subnet_ids = length(data.aws_subnets.private.ids) > 0 ? data.aws_subnets.private.ids : data.aws_subnets.public.ids
          }

          # IAM Role for EKS Cluster
          resource "aws_iam_role" "eks_cluster" {
            count = var.create_hub || var.create_spoke ? 1 : 0
            name  = "healthcp-eks-cluster-role"

            assume_role_policy = jsonencode({
              Version = "2012-10-17"
              Statement = [{
                Action = "sts:AssumeRole"
                Effect = "Allow"
                Principal = {
                  Service = "eks.amazonaws.com"
                }
              }]
            })
          }

          resource "aws_iam_role_policy_attachment" "eks_cluster_policy" {
            count      = var.create_hub || var.create_spoke ? 1 : 0
            policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
            role       = aws_iam_role.eks_cluster[0].name
          }

          # IAM Role for EKS Node Group
          resource "aws_iam_role" "eks_nodes" {
            count = var.create_hub || var.create_spoke ? 1 : 0
            name  = "healthcp-eks-node-role"

            assume_role_policy = jsonencode({
              Version = "2012-10-17"
              Statement = [{
                Action = "sts:AssumeRole"
                Effect = "Allow"
                Principal = {
                  Service = "ec2.amazonaws.com"
                }
              }]
            })
          }

          resource "aws_iam_role_policy_attachment" "eks_worker_node_policy" {
            count      = var.create_hub || var.create_spoke ? 1 : 0
            policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
            role       = aws_iam_role.eks_nodes[0].name
          }

          resource "aws_iam_role_policy_attachment" "eks_cni_policy" {
            count      = var.create_hub || var.create_spoke ? 1 : 0
            policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
            role       = aws_iam_role.eks_nodes[0].name
          }

          resource "aws_iam_role_policy_attachment" "eks_container_registry" {
            count      = var.create_hub || var.create_spoke ? 1 : 0
            policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
            role       = aws_iam_role.eks_nodes[0].name
          }

          # Hub Cluster (ArgoCD)
          resource "aws_eks_cluster" "hub" {
            count    = var.create_hub ? 1 : 0
            name     = local.hub_cluster_name
            role_arn = aws_iam_role.eks_cluster[0].arn
            version  = "1.29"

            vpc_config {
              subnet_ids              = local.subnet_ids
              endpoint_private_access = true
              endpoint_public_access  = true
            }

            tags = {
              Name        = local.hub_cluster_name
              Environment = "shared"
              Type        = "hub"
              ManagedBy   = "opsera"
            }

            depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy]
          }

          resource "aws_eks_node_group" "hub" {
            count           = var.create_hub ? 1 : 0
            cluster_name    = aws_eks_cluster.hub[0].name
            node_group_name = "${local.hub_cluster_name}-nodes"
            node_role_arn   = aws_iam_role.eks_nodes[0].arn
            subnet_ids      = local.subnet_ids

            scaling_config {
              desired_size = 2
              max_size     = 3
              min_size     = 1
            }

            instance_types = ["t3.medium"]

            tags = {
              Name = "${local.hub_cluster_name}-nodes"
            }

            depends_on = [
              aws_iam_role_policy_attachment.eks_worker_node_policy,
              aws_iam_role_policy_attachment.eks_cni_policy,
              aws_iam_role_policy_attachment.eks_container_registry,
            ]
          }

          # Spoke Cluster (Workload)
          resource "aws_eks_cluster" "spoke" {
            count    = var.create_spoke ? 1 : 0
            name     = local.spoke_cluster_name
            role_arn = aws_iam_role.eks_cluster[0].arn
            version  = "1.29"

            vpc_config {
              subnet_ids              = local.subnet_ids
              endpoint_private_access = true
              endpoint_public_access  = true
            }

            tags = {
              Name        = local.spoke_cluster_name
              Environment = "nonprod"
              Type        = "spoke"
              ManagedBy   = "opsera"
            }

            depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy]
          }

          resource "aws_eks_node_group" "spoke" {
            count           = var.create_spoke ? 1 : 0
            cluster_name    = aws_eks_cluster.spoke[0].name
            node_group_name = "${local.spoke_cluster_name}-nodes"
            node_role_arn   = aws_iam_role.eks_nodes[0].arn
            subnet_ids      = local.subnet_ids

            scaling_config {
              desired_size = 3
              max_size     = 5
              min_size     = 2
            }

            instance_types = ["t3.medium"]

            tags = {
              Name = "${local.spoke_cluster_name}-nodes"
            }

            depends_on = [
              aws_iam_role_policy_attachment.eks_worker_node_policy,
              aws_iam_role_policy_attachment.eks_cni_policy,
              aws_iam_role_policy_attachment.eks_container_registry,
            ]
          }

          # Outputs
          output "hub_cluster_endpoint" {
            value = var.create_hub ? aws_eks_cluster.hub[0].endpoint : "not created"
          }

          output "spoke_cluster_endpoint" {
            value = var.create_spoke ? aws_eks_cluster.spoke[0].endpoint : "not created"
          }

          output "hub_cluster_name" {
            value = local.hub_cluster_name
          }

          output "spoke_cluster_name" {
            value = local.spoke_cluster_name
          }
          EOF
          
          echo "Terraform configuration generated"

      - name: Terraform Init
        run: |
          cd /tmp/terraform
          terraform init

      - name: Terraform Plan
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          cd /tmp/terraform
          terraform plan \
            -var="create_hub=${{ inputs.create_hub }}" \
            -var="create_spoke=${{ inputs.create_spoke }}" \
            -out=tfplan
          
          echo "### ðŸ“‹ Terraform Plan" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          terraform show -no-color tfplan >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Terraform Apply
        if: inputs.action == 'apply'
        run: |
          cd /tmp/terraform
          terraform apply -auto-approve tfplan
          
          echo "### âœ… Infrastructure Created" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Hub Cluster:** \`${{ env.HUB_CLUSTER }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Spoke Cluster:** \`${{ env.SPOKE_CLUSTER }}\`" >> $GITHUB_STEP_SUMMARY
          
          # Get endpoints
          HUB_ENDPOINT=$(terraform output -raw hub_cluster_endpoint 2>/dev/null || echo "N/A")
          SPOKE_ENDPOINT=$(terraform output -raw spoke_cluster_endpoint 2>/dev/null || echo "N/A")
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Endpoints:**" >> $GITHUB_STEP_SUMMARY
          echo "- Hub: \`$HUB_ENDPOINT\`" >> $GITHUB_STEP_SUMMARY
          echo "- Spoke: \`$SPOKE_ENDPOINT\`" >> $GITHUB_STEP_SUMMARY

      - name: Terraform Destroy
        if: inputs.action == 'destroy'
        run: |
          cd /tmp/terraform
          terraform destroy -auto-approve \
            -var="create_hub=${{ inputs.create_hub }}" \
            -var="create_spoke=${{ inputs.create_spoke }}"
          
          echo "### ðŸ—‘ï¸ Infrastructure Destroyed" >> $GITHUB_STEP_SUMMARY
